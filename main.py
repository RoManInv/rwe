from argparse import ArgumentParser
import sys
import random
from modeltraining.preprocessing import load_embeddings_filtered_byvocab
import torch
import json
import numpy as np
import pandas as pd
import os
import pickle

import modeltraining.train_RWE as train_RWE
from model.RWE_Model import RWE_Model
from modeltraining.Tokenizer import Tokenizer

__BENCHMARK__ = './benchmark/'
__EXAMPLENUM__ = 5
__QNUM__ = 20

def trainmodel_getembedding():
    parser = ArgumentParser()
    parser.add_argument('-word_embeddings', '--input_word_embeddings', help='Input word embeddings path', required=True)
    parser.add_argument('-rel_embeddings', '--input_relation_embeddings', help='Input relation embeddings path', required=True)
    parser.add_argument('-output', '--output_path', help='Output path to store the output relational word embeddings or pretrained model', required=True)
    parser.add_argument('-hidden', '--hidden_size', help='Size of the hidden layer (default=2xdimensions-wordembeddings)', required=False, default=0)
    parser.add_argument('-dropout', '--drop_rate', help='Dropout rate', required=False, default=0.5)
    parser.add_argument('-epochs', '--epochs_num', help='Number of epochs', required=False, default=5)
    parser.add_argument('-interval', '--interval', help='Size of intervals during training', required=False, default=100)
    parser.add_argument('-batchsize', '--batchsize', help='Batch size', default=10)
    parser.add_argument('-devsize', '--devsize', help='Size of development data (proportion with respect to the full training set, from 0 to 1)', required=False, default=0.015)
    parser.add_argument("-lr", '--learning_rate', help='Learning rate for training', required=False, default=0.01)
    # parser.add_argument("-lr", '--learning_rate', help='Learning rate for training', required=False, default=0.01)
    parser.add_argument('-model', '--output_model', help='True for output model, False for output pretrained word embeddings', required=True, default=True)
    parser.add_argument('-hp', '--hyperparameters', help='Output path to store the output hyperparameters, until folder', required=True)

    args = vars(parser.parse_args())
    word_embeddings_path=args['input_word_embeddings']
    rel_embeddings_path=args['input_relation_embeddings']
    output_path=args['output_path']
    hidden_size=int(args['hidden_size'])
    dropout=float(args['drop_rate'])
    epochs=int(args['epochs_num'])
    interval=int(args['interval'])
    batchsize=int(args['batchsize'])
    devsize=float(args['devsize'])
    lr=float(args['learning_rate'])
    model = bool(args['output_model'])
    hp_path=args['hyperparameters']
    if devsize>=1 or devsize<0: sys.exit("Development data should be between 0% (0.0) and 100% (1.0) of the training data")

    print ("Loading word vocabulary...")
    pre_word_vocab=train_RWE.load_word_vocab_from_relation_vectors(rel_embeddings_path)
    print ("Word vocabulary loaded succesfully ("+str(len(pre_word_vocab))+" words). Now loading word embeddings...")
    matrix_word_embeddings,word2index,index2word,dims_word=train_RWE.load_embeddings_filtered_byvocab(word_embeddings_path,pre_word_vocab)
    if(type(matrix_word_embeddings).__module__=='numpy'):
        np.save(output_path + 'matrix_word_embeddings.npy', matrix_word_embeddings)
    else:
        pickle.dump(matrix_word_embeddings, open(output_path + 'matrix_word_embeddings.pkl', 'wb'))
    if(type(word2index).__module__=='numpy'):
        np.save(output_path + 'word2index.npy', word2index)
    else:
        pickle.dump(word2index, open(output_path + 'word2index.pkl', 'wb'))
    if(type(index2word).__module__=='numpy'):
        np.save(output_path + 'index2word.npy', index2word)
    else:
        pickle.dump(index2word, open(output_path + 'index2word.pkl', 'wb'))
    pre_word_vocab.clear()
    print ("Word embeddings loaded succesfully ("+str(dims_word)+" dimensions). Now loading relation vectors...")
    matrix_input,matrix_output,dims_rels=train_RWE.load_training_data(rel_embeddings_path,matrix_word_embeddings,word2index)
    print ("Relation vectors loaded ("+str(dims_rels)+" dimensions), now spliting training and dev...")
    random.seed(21)
    s1 = random.getstate()
    random.shuffle(matrix_input)
    random.setstate(s1)
    random.shuffle(matrix_output)
    matrix_input_train,matrix_output_train,matrix_input_dev,matrix_output_dev=train_RWE.split_training_data(matrix_input,matrix_output,devsize,batchsize)
    with open(output_path + 'inputtrainmatrix.txt', 'w') as f:
        for item in matrix_input_train:
            f.write(str(item))
    with open(output_path + 'inputmatrix.txt', 'w') as f:
        for item in matrix_input:
            f.write(str(item))
    # for item in matrix_output_train:
    #     with open(output_path + 'outputmatrix.txt', 'w') as f:
    #         f.write(str(item))
        
    matrix_input.clear()
    matrix_output.clear()
    print ("Done preprocessing all the data, now loading and training the model...\n")
    
    if hidden_size==0: hidden_size=dims_word*2
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print ("Device used: "+str(device))
    embedding_weights=torch.tensor(matrix_word_embeddings)
    matrix_word_embeddings.clear()
    tensor_input_train_1=torch.LongTensor([[x[0]] for x in matrix_input_train])
    tensor_input_train_2=torch.LongTensor([[x[1]] for x in matrix_input_train])
    matrix_input_train.clear()
    tensor_input_dev_1=torch.LongTensor([[x[0]] for x in matrix_input_dev])
    tensor_input_dev_2=torch.LongTensor([[x[1]] for x in matrix_input_dev])
    matrix_input_dev.clear()
    tensor_output_train=torch.FloatTensor(matrix_output_train)
    matrix_output_train.clear()
    tensor_output_dev=torch.FloatTensor(matrix_output_dev)
    matrix_output_dev.clear()
    hyperparams = dict()
    hyperparams['hidden_size'] = hidden_size
    hyperparams['dropout'] = dropout
    # hyperparams['dims_word'] = dims_word
    # hyperparams['dims_rels'] = dims_rels
    # hyperparams['embedding_weights'] = embedding_weights
    hyperparams_json = json.dumps(hyperparams)
    with open(hp_path + 'hyperparams.json', 'w') as f:
        json.dump(hyperparams_json, f)
    torch.save(dims_word, hp_path + 'dims_word.pt')
    torch.save(dims_rels, hp_path + 'dims_rels.pt')
    torch.save(embedding_weights, hp_path + 'embedding_weights.pt')
    print("Hyperparameters saved to " + hp_path)
    model, criterion = train_RWE.getRWEModel(dims_word,dims_rels,embedding_weights,hidden_size,dropout)
    print ("RWE model loaded.")
    optimizer = torch.optim.Adam(model.parameters(), lr)
    trainX1batches = train_RWE.getBatches(tensor_input_train_1.cuda(), batchsize)
    trainX2batches = train_RWE.getBatches(tensor_input_train_2.cuda(), batchsize)
    validX1Batches = train_RWE.getBatches(tensor_input_dev_1.cuda(), batchsize)
    validX2Batches = train_RWE.getBatches(tensor_input_dev_2.cuda(), batchsize)
    trainYBatches = train_RWE.getBatches(tensor_output_train.cuda(), batchsize)
    validYBatches = train_RWE.getBatches(tensor_output_dev.cuda(), batchsize)
    print ("Now starting training...\n")
    #with open(output_path + 'training_log.txt', 'w+') as f:
    # for x1, x2, y, i in zip(trainX1batches, trainX2batches, trainYBatches, range(1)):
            #np.savetxt(f, x1.cpu().numpy())
            #f.write("\n")
            #np.savetxt(f, x2.cpu().numpy())
            #f.write("\n")
            #np.savetxt(f, y.cpu().numpy())
            #f.write("\n")
            #f.write(str(len(x1)))
            #f.write("\n")
            #f.write(str(len(x2)))
            #f.write("\n")
            #f.write(str(len(y)))
            #f.write("\n")
            #f.write("=============================")
            #f.write("\n")
        # torch.save(x1, output_path + 'x1Tensor.pt')
        # torch.save(x2, output_path + 'x2Tensor.pt')
        # torch.save(y, output_path + 'yTensor.pt')
    output_model=train_RWE.trainEpochs(model, optimizer, criterion, (trainX1batches, trainX2batches, trainYBatches), (validX1Batches, validX2Batches, validYBatches), epochs, interval, lr)
    print ("\nTraining finished. Now loading relational word embeddings from trained model...")


    if(output_model):
        print ("\nSaving model...")
        torch.save(model.state_dict(), hp_path + 'model_state.pt')
        torch.save(model, hp_path + 'pretrained_model.pt')
        print ("Model saved to " + hp_path)
        print ("\nModel saved succesfully.")
    else:
        parameters=list(output_model.parameters())
        num_vectors=len(parameters[0])
        print ("Number of vectors: "+str(num_vectors))
        num_dimensions=len(parameters[0][0])
        print ("Number of dimensions output embeddings: "+str(num_dimensions))
        txtfile=open(output_path,'w',encoding='utf8')
        txtfile.write(str(num_vectors)+" "+str(num_dimensions)+"\n")
        if num_vectors!=embedding_weights.size()[0]: print ("Something is wrong in the input vectors: "+str(embedding_weights.size()[0])+" != "+str(num_vectors))
        for i in range(num_vectors):
            word=index2word[i]
            txtfile.write(word)
            vector=parameters[0][i].cpu().detach().numpy()
            for dimension in vector:
                txtfile.write(" "+str(dimension))
            txtfile.write("\n")
        txtfile.close()
        print ("\nFINISHED. Word embeddings stored at "+output_path)

    print('Testing with small DS')
    testword1 = 'germany'
    testword2 = 'german'
    testword3 = 'france'
    testword4 = 'french'
    testword5 = 'fresh'
    testword6 = 'rotten'
    cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)
    emb1 = model(torch.LongTensor([[word2index[testword1]]]).cuda(), torch.LongTensor([[word2index[testword2]]]).cuda())
    emb2 = model(torch.LongTensor([[word2index[testword3]]]).cuda(), torch.LongTensor([[word2index[testword4]]]).cuda())
    emb3 = model(torch.LongTensor([[word2index[testword5]]]).cuda(), torch.LongTensor([[word2index[testword6]]]).cuda())
    # with open('pretrainedmodel/embs.txt', 'w') as f:
    #     f.write(str(emb1))
    #     f.write("\n")
    #     f.write(str(emb2))
    # print(word2index[testword1])
    # print(word2index[testword2])
    # print(word2index[testword3])
    # print(word2index[testword4])
    print(cos(emb1, emb2))
    print(cos(emb1, emb3))

def loadmodel_calculateembedding():
    os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
    parser = ArgumentParser()
    parser.add_argument('-im', '--input_model', type=str, required=True, help='Path to the trained model')
    parser.add_argument('-hidden', '--hidden_size', help='Size of the hidden layer (default=2xdimensions-wordembeddings)', required=False, default=600)
    parser.add_argument('-dropout', '--drop_rate', help='Dropout rate', required=False, default=0.3)
    parser.add_argument('-epochs', '--epochs_num', help='Number of epochs', required=False, default=5)
    parser.add_argument('-interval', '--interval', help='Size of intervals during training', required=False, default=100)
    parser.add_argument('-batchsize', '--batchsize', help='Batch size', default=16)
    parser.add_argument('-devsize', '--devsize', help='Size of development data (proportion with respect to the full training set, from 0 to 1)', required=False, default=0.015)
    parser.add_argument("-lr", '--learning_rate', help='Learning rate for training', required=False, default=0.01)
    args = vars(parser.parse_args())

    hidden_size=int(args['hidden_size'])
    dropout=float(args['drop_rate'])
    epochs=int(args['epochs_num'])
    interval=int(args['interval'])
    batchsize=int(args['batchsize'])
    devsize=float(args['devsize'])
    lr=float(args['learning_rate'])

    # model = RWE_Model()
    # model.load_state_dict(torch.load(args['input_model']))
    dims_word = torch.load('pretrainedmodel/dims_word.pt')
    dims_rels = torch.load('pretrainedmodel/dims_rels.pt')
    embedding_weights = torch.load('pretrainedmodel/embedding_weights.pt')
    # model, criterion = train_RWE.getRWEModel(dims_word,dims_rels,embedding_weights,hidden_size,dropout)
    # model.load_state_dict(torch.load(args['input_model']))
    modelstate = torch.load('pretrainedmodel/model_state.pt')
    model = torch.load(args['input_model'])
    model.eval()
    matrix_word_embeddings = pickle.load(open('pretrainedmodel/RWE.modelmatrix_word_embeddings.pkl', 'rb'))
    word2index = pickle.load(open('pretrainedmodel/RWE.modelword2index.pkl', 'rb'))
    # print(model.state_dict())

    testfile = 'CountryToLanguage.csv'
    dataTable = pd.read_csv(os.path.join(__BENCHMARK__, testfile))
    example = dataTable.iloc[:__EXAMPLENUM__, :]
    XList = example.iloc[:, :-1].values.tolist()
    XList = [item for sublist in XList for item in sublist]
    Y = example.iloc[:, -1].values.tolist()
    Q = dataTable.iloc[__EXAMPLENUM__:__QNUM__, :-1].values.tolist()
    QY = dataTable.iloc[__EXAMPLENUM__:__QNUM__, -1].values.tolist()
    Q = [item for sublist in Q for item in sublist]

    tokenizer = Tokenizer()

    XList = [tokenizer.tokenize(sent) for sent in XList]
    Y = [tokenizer.tokenize(sent) for sent in Y]
    Q = [tokenizer.tokenize(sent) for sent in Q]

    # print(XList)
    # print(Y)
    # print(Q)

    # vocab = set()
    # for x in XList:
    #     vocab.add(x)
    # for y in Y:
    #     vocab.add(y)
    # print(XList)
    # print(Y)
    # print(vocab)
    # inputpath = 'traindata/RWE_default.txt'
    # matrix_word_embeddings,word2index,index2word,dimensions = load_embeddings_filtered_byvocab(inputpath, vocab)
    # print(matrix_word_embeddings[9])
    # print(word2index)
    # print(word2index.keys()[:10])
    # i = 0
    # for key in word2index.keys():
    #     print(key, word2index[key])
        # i += 1
        # if i == 10:
        #     break
    # print(word2index)
    # print(len(matrix_word_embeddings))
    # print(word2index[XList[0]], word2index[Y[0]])
    # print(word2index[XList[1]], word2index[Y[1]])
    tensor1 = torch.LongTensor([[word2index['germany']]]).cuda()
    tensor2 = torch.LongTensor([[word2index['berlin']]]).cuda()
    rel1 = model(tensor1, tensor2)
    # print(rel1)

    # tensor3 = torch.LongTensor([[word2index['china']]]).cuda()
    # tensor4 = torch.LongTensor([[word2index['beijing']]]).cuda()
    tensor3 = torch.LongTensor([[word2index['fresh']]]).cuda()
    tensor4 = torch.LongTensor([[word2index['rotten']]]).cuda()
    rel2 = model(tensor3, tensor4)
    # print(rel2)


    cos = torch.nn.CosineSimilarity(dim = 0, eps = 1e-6)
    print(cos(rel1, rel2))

    fresh_rotten = "-0.06377560123801232 -0.06785640195012092 0.18061776265967638 -0.12278739660978318 \
    -0.13613999634981155 0.34794980064034464 -0.11092540137469768 -0.1337531954050064 0.15914519876241684 \
    -0.04939216081984341 0.09565780013799667 -0.18977799713611604 -0.2148580014705658 -0.12611800134181977 \
    -0.005786198750138282 -0.02302220053970814 -0.07497780043631792 -0.1712304025888443 0.1103125992231071 \
    -0.10277400091290474 0.03205010090023279 0.05440244027413428 0.18375839591026305 0.09851380288600922 \
    -0.009142398834228516 0.10471360012888908 0.040396998450160024 -0.06133980005979538 -0.19885099232196807 \
    0.2900760069489479 -0.002022659592330456 0.03248557755723595 0.06229579970240593 0.02807999923825264 \
    0.15127059817314148 -0.1843255989253521 -0.006533000990748406 0.11713740229606628 -0.073249801248312 \
    0.1226021982729435 0.07465300187468529 -0.22438523662276566 0.025985920056700706 0.2150714010000229 \
    0.03770139962434769 -0.006168200820684433 0.2766460031270981 -0.03924224078655243 -0.055173999071121214 \
    0.006824798882007599 -0.03385139890015125 0.0016843996942043304 -0.17512640058994294 0.008766399510204793 \
    0.25682179927825927 0.13628600761294365 -0.04755902187898755 0.0064283153275027875 0.15666900277137757 \
    -0.13240860104560853 0.047386359795928 -0.009380660578608513 0.09826980084180832 -0.07398759871721268 \
    -0.021722021326422692 -0.19071120247244835 0.018187718559056522 0.24452399611473083 -0.029095999896526337 \
    -0.10909979790449142 0.17930445082020013 0.10280160382390022 0.006461998820304871 0.05040460005402565 \
    -0.18474439904093742 0.02176779955625534 -0.035475998371839526 0.17682191934436559 -0.19768539816141129 \
    -0.01859320104122162 0.07600540108978748 0.17301080003380775 0.010053057875484227 -0.2141940023750067 \
    -0.06375552045647055 -0.00452598063275218 0.16612064288929104 0.025375798903405668 -0.20376000106334685 \
    -0.10699279941618442 0.0018119990825653076 -0.08318140096962452 0.12969359885901213 0.13157459907233715 \
    -0.01245804037898779 -0.027313200756907463 0.10724259912967682 -0.0287385992705822 0.24664000049233437 \
    0.018132000416517257 -0.13330260030925273 -0.018451540730893612 0.03685260266065597 -0.04092405866831541 \
    0.024686200357973576 -0.016005402058362962 0.03401339948177338 -0.01573319882154465 0.0976982019841671 \
    -0.09532159864902497 -0.016870799474418163 -0.056477976003998266 0.04801059737801552 -0.08962480053305626 \
    0.06754680220037698 0.15818540006875992 -0.025649801827967167 -0.11728840097784995 0.11634600311517715 \
    0.04133012024685741 -0.07355597864370793 -0.060852402448654176 -0.028771200217306614 0.1549919992685318 \
    -0.12251139879226684 -0.022375400364398956 0.21708399951457977 -0.1732352003455162 0.1180742010474205 \
    0.04154600016772747 0.20880859903991222 -0.03767979964613914 0.13587640225887299 0.06484380066394806 \
    -0.008864880399778485 -0.07340000085532665 0.214130000770092 -0.03403700143098831 0.006872320012189448 \
    -0.15696299821138382 -0.07080439925193786 0.2828559949994087 -0.06245834343135357 0.2417521983385086 \
    0.13171400129795074 0.07699520029127598 0.2608220010995865 0.03610780015587807 0.10132980123162269 \
    -0.04021899998188019 0.08339780233800412 0.052282598614692685 0.21856000125408173 0.024554800242185593 \
    0.15909359492361547 0.049156400933861734 0.07330443821847439 -0.08045103922486305 -0.14452979788184167 \
    0.006984139326959848 -0.20511799603700637 0.002833339851349592 -0.07405139841139316 -0.0059631988406181335 \
    0.23017079569399357 -0.07931500002741813 -0.0022717999294400216 -0.013608597964048386 -0.056282800063490866 \
    0.023171599209308624 0.06314340233802795 0.06838500034064054 -0.10713939815759659 0.06574639938771724 \
    0.08464719802141189 -0.15705846091732384 -0.06482080072164535 0.04698200225830078 0.09685173965990543 \
    0.03929679766297341 -0.2280413970351219 0.06808913031127303 -0.018817999213933945 -0.22495879642665387 \
    -0.2676176007837057 0.07754679918289184 0.06017719935625791 0.026339201629161833 -0.023899999260902405 \
    -0.0005745818838477134 -0.14321500211954116 -0.08186554280109704 -0.052084797993302345 -0.09267002018168569 \
    0.07356880605220795 -0.13853259906172752 -0.07658620160073042 -0.030316420830786228 0.1445817705243826 \
    -0.2317059963941574 0.2405868012458086 0.025874840561300516 -0.009670199826359749 -0.1255566030740738 \
    -0.04054120033979416 -0.1269154004752636 -0.07821047916077077 -0.11069419849663972 0.07569745955988764 \
    -0.09774660468101501 -0.15719179958105087 0.09785400182008744 -0.015729400515556335 -0.0534697987139225 \
    0.12641480267047883 0.11053360179066658 0.01566291986964643 -0.07557800635695458 0.05108000189065933 \
    -0.09036719799041748 0.15396699942648412 0.06501959543675184 -0.02553799916058779 -0.026882800459861755 \
    0.20645319819450378 0.07472440004348754 -0.004740800708532333 0.043976600468158725 -0.0021179988980293274 \
    -0.05375500172376633 0.04409979917109012 0.012081798911094666 -0.0731097985059023 -0.15731339305639266 \
    0.1089569978415966 0.09528559818863869 0.1060018421150744 -0.17337840497493745 0.0005883991718292237 \
    -0.13214479815214872 0.08728240057826042 -0.10987420007586479 0.04427489708177745 0.08991800155490637 \
    0.0307010801974684 -0.09979681838303804 -0.030030998587608337 0.07037360146641732 0.1252960187382996 \
    0.08624913953244687 0.018985400348901747 -0.013756000995635986 -0.0888078011572361 -0.13834053929895163 \
    -0.04176180101931095 -0.13501039743423462 0.05329440012574196 0.024851399660110473 0.061729401163756845 \
    -0.06209240108728409 -0.08478620126843453 0.18648139983415604 0.2723780035972595 0.3713199943304062 \
    0.06699139922857285 0.037641181517392396 -0.0298505999147892 -0.02204279825091362 0.1343094002455473 \
    0.16395820155739785 -0.047920739464461806 -0.03534359894692898 -0.022939597070217133 -0.02785577652975917 \
    -0.10899659842252732 0.024072401225566864 0.019510798156261444 0.1386731404811144 0.0028463996946811675 \
    -0.06617039963603019 -0.009488601982593537 0.12188920006155968 -0.03753960058093071 0.029703337862156332 \
    0.06211328096687794 0.11650202153250575 0.1277493990957737 -0.08780520185828208 -0.15254195807501675 \
    0.1925929993391037 -0.05928299650549888 -0.14749441877938807 -0.16375999748706818 -0.039694001711905 \
    -0.2939519971609116 0.08541699945926666 -0.005393202602863312 0.1128253996372223 -0.05797660034149885 \
    -0.14221079871058465".split(' ')
    germany_berlin = "-0.06282061518868431 -0.08724112895750163 -0.008859959478017921 -0.06368025486353061 \
    -0.1013573255054277 0.17905025007943484 -0.013137434221121468 -0.11817432113296814 0.11056747280396526 \
    -0.00739817849127205 0.1263998086926527 -0.042477047375400624 -0.1480092867289921 -0.12576830562062094 \
    0.04970903851194065 0.08143301634949057 0.00014838737931724817 -0.09659003191333435 0.04346716113404948 \
    -0.022740745246987503 0.04924442664081935 -0.0601061653618046 0.014698657578589234 0.043108600239119776 \
    -0.07197682225239509 0.02129910582223721 -0.04688515464540286 0.010706445414361875 -0.18889107526053334 \
    0.2142334311532873 -0.04010156033638615 -0.03803496378300427 -0.07923789895425884 0.021243350546225715 \
    0.047511373759106815 -0.12890060050298388 0.0005081185929664922 0.028971446408673638 -0.05154755005049183 \
    0.18526404085974282 0.13903548451009032 -0.14984872354316375 0.06472565632383562 0.112444876786618 \
    -0.07864308646179251 -0.08868268874067034 0.339641325297955 -0.05025637979744595 -0.022671163598144223 \
    0.06522398094619197 -0.09918591713867964 -0.07714012343802719 -0.16919016159565195 0.032705274818806605 \
    0.27114011473224303 -0.11545044840470445 -0.004673374852282304 0.07098039411448746 0.19952612644044887 \
    -0.16349602412521907 -0.03871449964667098 -0.00991452436569402 0.07356107209926158 -0.01722387808151823 \
    -0.040957215299554005 -0.04340973862731333 0.04335673903346194 0.23812358297228184 0.07283901089905948 \
    -0.07525196008647707 0.12193499161140567 0.09491382892411143 -0.00449877530871495 0.07640651285496586 \
    -0.25918167452102986 -0.08396338268070463 -0.11078112450059535 0.07942456308475596 -0.07082887755037487 \
    0.08425283340558398 0.020352104273829868 0.2846300689426435 0.012652594467435692 -0.16436004608777113 \
    -0.0989109246442773 -0.01142742135511797 0.22485622953337717 0.04809235787085127 -0.2908704876286747 \
    0.07243085477159662 0.18231191175466185 -0.1498801502715025 0.10453376281230894 0.07330771265624363 \
    0.05412301061105976 0.07985096480118406 0.08947476531144534 -0.005564286030736019 0.130315392992044 \
    -0.028371474868364607 0.004460233817818217 -0.03147163498883885 -0.052367488519043624 0.002608315204826308 \
    -0.010207252752497068 -0.07292607909729883 -0.07963215353683176 0.04618650091839636 0.09953933329740823 \
    0.0488406665766977 -0.011046837592848939 -0.09450329650313784 0.02898107953812045 -0.020185651859638554 \
    -0.008826954240497694 0.12089972945834268 -0.08592394786619807 -0.10764047882587385 0.1759250161036327 \
    -0.05257214308325092 0.02934941299949998 -0.022362038686038824 0.0007941980073584398 0.15472672874830676 \
    -0.1259961084407337 0.079107618687841 0.18265689831118806 -0.13547152929668435 0.15808300399057115 \
    0.050389000057902006 0.20702162187005907 -0.10137842141702695 0.058853982552115346 -0.06024606626240541 \
    -0.04640260010583508 -0.03668388742540153 0.1406576566251018 -0.025362542990918953 0.0007021808410394588 \
    -0.12153927403787149 -0.15999263215158407 0.1595369980611439 -0.07778391026683654 0.0877607325466415 \
    0.1552536541164997 0.005563364285793286 0.20961148744131286 0.003689737706875227 0.07095678522671543 \
    -0.0882803178272475 0.004713291527955267 0.030270114427975194 0.09726074779983755 0.07019166283372734 \
    0.06979423290231777 0.053901876367538605 0.1277990256155141 0.0479645388601721 -0.13009648126139228 \
    0.001652543049207676 -0.2151624924585326 0.007541236570017213 0.07825575073804097 0.04153733115521054 \
    0.04644669334573726 -0.03595508629955562 -0.026373359698474368 -0.07920265148024049 -0.14336801752126271 \
    0.037105348914075194 0.017000136473510244 0.11435748207397811 -0.1413217227242471 0.03784036451700784 \
    -0.042231634496186894 -0.047855870191004395 -0.011260671045379617 0.09132001272422956 0.010527528389942814 \
    0.08668436326351978 -0.19060820738275558 -0.025106280376171228 0.05308317681575528 -0.04712320072210289 \
    -0.17711025915219106 0.015352245361021187 -0.007680454737105903 -0.0946701757521544 -0.05042815247073329 \
    0.09305828229726547 -0.1388934214385925 0.005904576473168914 -0.08633298453180473 0.0022988083334320826 \
    0.0034370911532863205 -0.0553592439071261 -0.0602818580572701 -0.006754229375224999 0.043941072452105996 \
    -0.12171844041798563 0.09079322558909263 -0.06991993829017792 -0.03886148365177516 -0.11265166091239028 \
    -0.09084342653225319 -0.1274232914694134 -0.0789409575955705 0.03560755925758859 0.031397832892531126 \
    -0.13152650700190754 -0.13919373286649242 0.11847788134328784 0.10034048853402155 -0.014794534639773729 \
    0.16960421956423632 0.09926767127994733 0.09145555443304933 -0.08564949761531755 0.002699911624143511 \
    -0.14773132378357065 0.058765718034653226 -0.09719229711866255 -0.07087860140475423 0.010344017089162647 \
    0.08444603937414961 0.1682283655994006 -0.03400277893027195 0.046727251905680736 -0.062219630406608765 \
    -0.11124934572352341 0.08045048583491711 -0.004233105898487002 0.008506014910867664 -0.12086081671466696 \
    0.04254866531069317 0.11658677551092089 0.05703815563579165 -0.10863339171123457 0.11296830014184793 \
    -0.05276083865678079 0.14679910326359255 -0.032563946177160186 0.059495648792897884 0.02044805704294447 \
    0.011418509486788462 -0.0214967615366073 -0.06779601433033355 0.05221580711834686 0.001067913860167775 \
    0.042125259071905875 0.0015305581610998824 0.02818376283547416 -0.17259820445053647 0.045557124115207334 \
    -0.02857631177678619 -0.07699536589398064 0.12368421896517517 0.035474410511562186 0.048378843367814076 \
    0.036494693998890014 -0.026940194901890632 0.08731286628971384 0.18673933965191708 0.30267764177596523 \
    0.0232746830184535 -0.10617305149331406 0.08557514451748657 0.03010734000493178 -0.06117825478393458 \
    0.011464370649484443 -0.0654344002741207 -0.035344711398610555 0.07897543719782585 -0.03277687328248957 \
    -0.028892247248087533 -0.03682996940197146 0.009722928688331876 0.061802493589464325 0.01260769584273273 \
    -0.06883250125874471 0.02599033325991021 0.23028594496865515 -0.09077156737684436 -0.025284078402088555 \
    0.04031858670319734 0.009805501169451432 0.10108535191349585 -0.09955546756062528 -0.24642450173800434 \
    0.10251893241659416 0.048040560758170904 -0.062490497463290835 -0.14904777593548596 -0.0071403928418177125 \
    -0.2118530988505638 0.10057068715379806 0.1175260616533694 0.20101287681356048 -0.0504712230688917 \
    -0.03066836348935001".split(' ')
    china_beijing = "-0.07635857918552019 -0.07405002944772959 0.022606491093604738 -0.05643856407532644 \
    -0.08108868869800641 0.15138633679339278 -0.04468251710366893 -0.11292810802205833 0.10579423698158019 \
    -0.0362527198086266 0.1201502670965774 -0.045668077131439666 -0.1470557331696014 -0.13983635129216568 \
    0.053048013799815316 0.07809539681681107 -0.04156110483075456 -0.11074444656345386 0.06567505362443349 \
    -0.054658374165937086 0.036541808819920495 -0.05109642315467968 0.011190853217951473 0.03522237470986477 \
    -0.07436330441001199 0.027127044474015944 -0.050998372779668524 -0.02603552883950013 -0.15477468195183477 \
    0.20580367154648457 -0.03310249730931059 -0.03412700913292754 -0.058036180284492755 0.007328738554680223 \
    0.016873332214573513 -0.123381508374349 -0.0730709998345484 -0.005975873164246074 -0.020645907208021213 \
    0.17300286247102972 0.10948175104026678 -0.13885081919950565 0.06762290702188821 0.12396599700614236 \
    -0.08301037370939406 -0.09157258084286325 0.35554389506158074 -0.04366969701841045 0.008678214618972339 \
    0.03610611535251703 -0.03583360665496829 -0.14104484201264483 -0.16760104833345646 0.015172659666194896 \
    0.265132387837296 -0.09475340811452017 0.017371822711308514 0.05104824950350741 0.16297551443261135 \
    -0.1612219312852251 -0.035186831177673204 -0.028830275391342346 0.06910481032735286 -0.035982909089196324 \
    -0.044458298018056557 -0.0858214527197629 0.02255300126327935 0.22814300557512646 0.057688097321221425 \
    -0.10166597218790194 0.08034921949103771 0.08146850448265654 0.010594439078221215 0.08141178823617519 \
    -0.2598851964673756 -0.09256669093406454 -0.09589815055139508 0.08255576438748181 -0.05979629100243071 \
    0.07579408950623825 0.03349834417212156 0.2550508987211959 0.03940193318740429 -0.16387721546043577 \
    -0.08514244401142464 -0.01912316820772654 0.17524244018012855 0.07088078180996758 -0.27500663819502136 \
    0.08209996418800529 0.17013156502904228 -0.1594854414664221 0.1037995463516413 0.07033600824315693 \
    0.056130773530287635 0.055885388115973926 0.07725362320936759 -0.0076580807974628105 0.1338466694686827 \
    0.009242925713330954 -0.011663702932845257 -0.03189611945912835 -0.04322153163330293 -0.014678500598354198 \
    -0.008693355291131993 -0.04798516532170252 -0.07719955107721754 0.0667795420017045 0.11385067875476025 \
    0.04732546557764653 -0.0040610605365981305 -0.06375062041511846 0.030183553501069327 -0.009473715801762086 \
    -0.0159378730381883 0.10631245455536918 -0.07699255163170399 -0.07132921172503304 0.16524708145540778 \
    -0.06911285017471418 0.00666047634977511 -0.03197131912634482 -0.008133937827422054 0.15472449804929203 \
    -0.11982705989213358 0.09977421140841727 0.1927237097799294 -0.14124234085348333 0.1598887694351347 \
    0.042558269026003126 0.18029477563757043 -0.09072995590349942 0.02979260014828864 -0.06067656079877046 \
    -0.02907837453643388 -0.05009727540279358 0.13280566276794512 -0.060597893584469174 0.01592903150253867 \
    -0.11435177146907784 -0.12484066998217921 0.14893931043642078 -0.1002834081070331 0.12267446462407784 \
    0.14781010254200655 0.02077010495255452 0.14981161985103897 0.008784821706392124 0.06532242246765115 \
    -0.08402027915869822 0.016399149150522062 0.07175088633766522 0.08082848077718507 0.06584595591218317 \
    0.10288944835209414 0.04500081003838745 0.14959267298926182 0.08062092393127128 -0.10838129563023892 \
    -0.00406335757074208 -0.2061476724908909 0.026230678567243714 0.08845145189573055 0.04646326308997317 \
    0.06353983810158473 -0.04855630353571945 -0.023657920660937984 -0.09958972819161145 -0.17250957463823924 \
    0.042216398522209264 0.008292828664578466 0.13696165064061766 -0.1301303278267722 0.03759319534188603 \
    -0.009231011387798077 -0.0319152106502189 -0.03831410948991573 0.0651138944274917 0.01873818153014328 \
    0.0778773185802478 -0.16967405969427657 -0.018716274161822092 0.04673456534757572 -0.05651851676043908 \
    -0.15186561965242215 0.00010970145144786592 -0.012863318644406966 -0.08988854366728162 -0.09040675227724722 \
    0.07953261467831095 -0.17124500009933452 0.0007512746268056618 -0.07850430120719264 -0.0012756336758687416 \
    -0.032267938837053894 -0.07478026819179807 -0.06939174855315096 0.006231366951273766 0.040608618127818685 \
    -0.09334386209111242 0.1291773997786195 -0.026565928965828496 0.005130436631684752 -0.10827429410066929 \
    -0.08532236061064068 -0.13396490702546532 -0.05452585578071627 0.02165055264594846 0.04172586691874835 \
    -0.12071055254526386 -0.13134589228444532 0.07946884832700263 0.10527210645358556 -0.015558560190120314 \
    0.11687849758139944 0.11312822473607256 0.11626061710509263 -0.09216872356393015 0.0009729652109595685 \
    -0.1706317480859206 0.0673040942077217 -0.08936553514065358 -0.0697526164787778 0.02644136616671663 \
    0.04988705029845215 0.15982663696434116 -0.031941883267989545 0.045785523336010395 -0.04410423819142522 \
    -0.09971191371226941 0.07232629925711706 0.0019867245175073517 -0.017955112629030216 -0.14073061935081532 \
    0.060028163102092115 0.12665110034542834 0.09906869862040465 -0.11464057196645304 0.12247353640944467 \
    -0.058938606667568436 0.16132928441455793 -0.02636293441051638 0.060694559789213134 0.0510131874077499 \
    -0.002988653009707856 -0.024525456678754467 -0.05717603363095788 0.07414892485627785 0.01649113839165288 \
    0.015876485351759596 0.007351882302918917 0.014236123444215223 -0.13822976922910246 0.06743828048097822 \
    -0.009742432046629846 -0.04450285747542943 0.10710754137818262 0.028596129552820645 0.06552171329097534 \
    0.04427706717447443 -0.03634378915327471 0.09186543304228759 0.1855395988012979 0.2776866412427815 \
    0.038992539542526024 -0.12294587961328378 0.047971153370857146 0.022788062685245348 -0.059848362303416267 \
    0.0570835061571816 -0.064031705332966 -0.0040911778812394274 0.08919971176757928 -0.0264286700640715 \
    -0.03793162032945581 -0.03408033861158754 0.009217534507876903 0.06617054240307674 0.015232417385780348 \
    -0.07962393894620155 0.046108914627796184 0.19896338354956125 -0.07948450841182007 -0.05293856030641118 \
    0.021380957313270447 -0.01773381316182361 0.09387088327503837 -0.11112647522943998 -0.2562868407819108 \
    0.09973082158254658 -0.017858450439958026 -0.08869436260990175 -0.09530723077524804 0.03965628450757484 \
    -0.2340747032235469 0.09623151546788491 0.1304843091204 0.1712862814225863 -0.048009233422562865 \
    -0.05803377450829645".split(' ')
    germany_german = "-0.06747994094131919 -0.043182569539532985 0.03443403170123195 -0.03406880670644617 \
    -0.10590530920570561 0.16277501431327446 -0.03233244708959076 -0.08538752591222175 0.1091079619961126 \
    -0.030599656653583236 0.08371807465613583 -0.014843040270186738 -0.09066934669555425 -0.1113253187188059 \
    0.0749355949870681 0.10590945999106574 -0.038353285962756056 -0.09868963502631119 0.07334130262838232 \
    -0.01920657629910732 0.032320153139270676 -0.035721825524445 0.06942682113432101 0.05398969587040118 \
    -0.08019527812499329 0.023072162576132003 -0.04169300581522404 -0.006844964080230985 -0.151859882527859 \
    0.18985525327440897 -0.023038735430268648 -0.056215007306895944 -0.05159447954781863 0.007758786687866639 \
    -0.0041355807520632145 -0.07296577279612632 -0.043250058406837405 -0.010741189252577589 -0.022802436988942673 \
    0.1572452570167034 0.06997687386318523 -0.11201205525796962 0.07960133485857229 0.13163517251846313 \
    -0.08738813270138045 -0.07000572952105288 0.3055852886193398 -0.04931306581168709 0.07588234947945272 \
    0.03394340477232295 -0.057494451572664494 -0.11900250264906141 -0.17520280808747976 0.008322866887057112 \
    0.26100773901258645 -0.10709282080306824 0.03785040486966427 0.0675871391191181 0.10666392307853823 \
    -0.15083933134939592 -0.03318112395896965 -0.03126231874380402 0.06780233623349254 -0.02929041839408852 \
    -0.046750678623370484 -0.0921436513730069 0.004979763205291118 0.1929558875584553 0.06314744014818006 \
    -0.08886149836345092 0.03623013896105075 0.11023795992161077 0.04110478941282218 0.08734350363253973 \
    -0.19645253504743787 -0.07482034688027216 -0.09452433641071879 0.056542232434038 -0.07391831296452685 \
    0.0747733195743069 0.07933780444095642 0.2523222078561427 -0.016874703501855725 -0.1559849130942384 \
    -0.10451933205085664 0.03440842407303529 0.18585690719027367 0.1154216556830465 -0.2666709845620868 \
    0.04413321149482036 0.1653628695668969 -0.17224683638244004 0.1116299165246134 0.06789663591516319 \
    0.051843435196877244 0.05606622065645554 0.07117445686707292 -0.02613079248836917 0.18990539206203474 \
    0.0014088844056471233 -0.03602027007903634 -0.0399718449004752 -0.034063317480489005 -0.019649450037827792 \
    0.002919091445703559 -0.025301670062583714 -0.05620698438518904 0.05893931919930379 0.12083696619531363 \
    0.01959556216531658 0.016268367600828625 -0.06012506504304785 0.02296501659161905 0.003693629119446367 \
    0.05312361551550166 0.06999866677215531 -0.10045962121388081 -0.04960043307931003 0.1789239157966733 \
    -0.038650319164149294 -0.007491486874855798 -0.049023243719765616 -0.01571006088898525 0.17376298038034302 \
    -0.07529071447427636 0.11032932942668101 0.1801755667573856 -0.20531148772958124 0.17502632934237816 \
    0.044828425318598916 0.1610371947326349 -0.07870867954083281 0.047449346060142616 -0.060195366222813414 \
    -0.023254217543822814 -0.05909378564424896 0.13700534929369426 -0.03405706402600976 0.04428682177627977 \
    -0.13520459044304733 -0.11373785939908712 0.15398645749136503 -0.0867824051340756 0.11183271180439493 \
    0.14149712954409666 0.02311291044172781 0.16603415211684885 -0.0032449492013240876 0.05923589440372223 \
    -0.05186956804171194 0.021962718915822146 0.09573389342538427 0.046751589997492723 0.021111969120446755 \
    0.10758955765871603 0.0030043537838920194 0.13733260378192919 0.09489772018875262 -0.1141672741190136 \
    -0.0010959866286232966 -0.21811796709035836 0.02075650826154812 0.05639255813333481 0.0756953180454039 \
    0.07151400316513094 -0.015130550070450536 -0.00781935476935063 -0.13113011178076156 -0.18402307458015577 \
    0.05079056426523576 0.019928937581285108 0.13855333433480466 -0.10689851881274882 0.045155397400848535 \
    -0.008305840368971792 -0.016098198404843122 -0.03008208315298694 0.0789161723483383 0.03444551403726147 \
    0.07669763010963421 -0.2146578185607997 -0.033809921545431394 0.0445926567115219 -0.09871763257491625 \
    -0.13021514738052217 0.024126408184700857 -0.04762457493298305 -0.05915886779796432 -0.08186689269688593 \
    0.058591318390952704 -0.1556572303679632 0.018843298921964628 -0.07413753663109446 -0.005793019209117819 \
    -0.06395314032385674 -0.06494039090606535 -0.06917393487063014 -0.024029349840919752 0.08378950153428606 \
    -0.12379021062729414 0.11140525159235881 -0.006351163556203705 -0.0077219941240524595 -0.11255951260220903 \
    -0.08582251901079345 -0.13344112987148155 -0.01695629534681172 -0.030389420622332988 0.05809221688751565 \
    -0.10838342965959576 -0.09480088348948643 0.0762776299737041 0.10328228969714708 -0.04201049301223377 \
    0.09938108954383017 0.12507386864300674 0.10440543844079347 -0.07584281742120036 0.0778967713675178 \
    -0.16867028019435976 0.03022802045873483 -0.08877758678310275 -0.0933766886722715 0.060499484316656735 \
    0.05160574481778077 0.11273888485095217 -0.00629761116289737 0.0710398739821439 -0.035560767746660664 \
    -0.07193278096782459 0.07297096216220322 -0.00724606496755087 -0.03472194861133532 -0.1210563049858415 \
    0.057113341239297476 0.16565811382901074 0.07874199963594056 -0.1296972171056977 0.11822721655704321 \
    -0.06493180643135239 0.13930857567193786 -0.03424442794494098 0.08468492414672887 0.030590924310072466 \
    0.010520773791437428 -0.04572381397582955 -0.04593025544464143 0.04711959558854928 0.04977062583715797 \
    0.008091584141556545 0.00518913285205718 0.015196242162081347 -0.15440018893381338 0.01477741822100284 \
    0.01128320896741116 -0.03302683178172898 0.08858977960752261 0.05576105369246598 0.07974541084989509 \
    0.012198985462868352 -0.05909014432072781 0.1013534460217352 0.1821444500856377 0.27669271000860446 \
    0.027118185274347697 -0.10609399678399377 0.01943705669492717 0.02452283258493206 -0.056936445720667875 \
    0.08628014550087301 -0.08086996824757675 0.02318956625706621 0.09354908342841742 -0.05840980070881635 \
    -0.07340019954941197 -0.029512655164787783 0.038304374026778165 0.1020823202309442 0.014574800771394173 \
    -0.051556779047009886 0.0361489385484358 0.15871218870991982 -0.09929301078626994 -0.01878642425707002 \
    0.017942854375520297 -0.03716860039533156 0.1032960227931796 -0.1202210405657447 -0.23404143273403716 \
    0.1365643397606046 0.008414470449772633 -0.14196520423813694 -0.0983026755669912 0.06884104216742606 \
    -0.20796302808098285 0.14372745464365724 0.16559802615141006 0.15835736819737953 -0.053516890354376706 \
    -0.0871673172580796".split(' ')


    # for name, param in model.named_parameters():
    #     print(name, param.size())
    # print(modelstate)
    

def checkTensor():
    __PATH__ = './pretrainedmodel/'
    __MODEL__ = 'pretrained_model.pt'
    fresh_rotten = "-0.06377560123801232 -0.06785640195012092 0.18061776265967638 -0.12278739660978318 \
    -0.13613999634981155 0.34794980064034464 -0.11092540137469768 -0.1337531954050064 0.15914519876241684 \
    -0.04939216081984341 0.09565780013799667 -0.18977799713611604 -0.2148580014705658 -0.12611800134181977 \
    -0.005786198750138282 -0.02302220053970814 -0.07497780043631792 -0.1712304025888443 0.1103125992231071 \
    -0.10277400091290474 0.03205010090023279 0.05440244027413428 0.18375839591026305 0.09851380288600922 \
    -0.009142398834228516 0.10471360012888908 0.040396998450160024 -0.06133980005979538 -0.19885099232196807 \
    0.2900760069489479 -0.002022659592330456 0.03248557755723595 0.06229579970240593 0.02807999923825264 \
    0.15127059817314148 -0.1843255989253521 -0.006533000990748406 0.11713740229606628 -0.073249801248312 \
    0.1226021982729435 0.07465300187468529 -0.22438523662276566 0.025985920056700706 0.2150714010000229 \
    0.03770139962434769 -0.006168200820684433 0.2766460031270981 -0.03924224078655243 -0.055173999071121214 \
    0.006824798882007599 -0.03385139890015125 0.0016843996942043304 -0.17512640058994294 0.008766399510204793 \
    0.25682179927825927 0.13628600761294365 -0.04755902187898755 0.0064283153275027875 0.15666900277137757 \
    -0.13240860104560853 0.047386359795928 -0.009380660578608513 0.09826980084180832 -0.07398759871721268 \
    -0.021722021326422692 -0.19071120247244835 0.018187718559056522 0.24452399611473083 -0.029095999896526337 \
    -0.10909979790449142 0.17930445082020013 0.10280160382390022 0.006461998820304871 0.05040460005402565 \
    -0.18474439904093742 0.02176779955625534 -0.035475998371839526 0.17682191934436559 -0.19768539816141129 \
    -0.01859320104122162 0.07600540108978748 0.17301080003380775 0.010053057875484227 -0.2141940023750067 \
    -0.06375552045647055 -0.00452598063275218 0.16612064288929104 0.025375798903405668 -0.20376000106334685 \
    -0.10699279941618442 0.0018119990825653076 -0.08318140096962452 0.12969359885901213 0.13157459907233715 \
    -0.01245804037898779 -0.027313200756907463 0.10724259912967682 -0.0287385992705822 0.24664000049233437 \
    0.018132000416517257 -0.13330260030925273 -0.018451540730893612 0.03685260266065597 -0.04092405866831541 \
    0.024686200357973576 -0.016005402058362962 0.03401339948177338 -0.01573319882154465 0.0976982019841671 \
    -0.09532159864902497 -0.016870799474418163 -0.056477976003998266 0.04801059737801552 -0.08962480053305626 \
    0.06754680220037698 0.15818540006875992 -0.025649801827967167 -0.11728840097784995 0.11634600311517715 \
    0.04133012024685741 -0.07355597864370793 -0.060852402448654176 -0.028771200217306614 0.1549919992685318 \
    -0.12251139879226684 -0.022375400364398956 0.21708399951457977 -0.1732352003455162 0.1180742010474205 \
    0.04154600016772747 0.20880859903991222 -0.03767979964613914 0.13587640225887299 0.06484380066394806 \
    -0.008864880399778485 -0.07340000085532665 0.214130000770092 -0.03403700143098831 0.006872320012189448 \
    -0.15696299821138382 -0.07080439925193786 0.2828559949994087 -0.06245834343135357 0.2417521983385086 \
    0.13171400129795074 0.07699520029127598 0.2608220010995865 0.03610780015587807 0.10132980123162269 \
    -0.04021899998188019 0.08339780233800412 0.052282598614692685 0.21856000125408173 0.024554800242185593 \
    0.15909359492361547 0.049156400933861734 0.07330443821847439 -0.08045103922486305 -0.14452979788184167 \
    0.006984139326959848 -0.20511799603700637 0.002833339851349592 -0.07405139841139316 -0.0059631988406181335 \
    0.23017079569399357 -0.07931500002741813 -0.0022717999294400216 -0.013608597964048386 -0.056282800063490866 \
    0.023171599209308624 0.06314340233802795 0.06838500034064054 -0.10713939815759659 0.06574639938771724 \
    0.08464719802141189 -0.15705846091732384 -0.06482080072164535 0.04698200225830078 0.09685173965990543 \
    0.03929679766297341 -0.2280413970351219 0.06808913031127303 -0.018817999213933945 -0.22495879642665387 \
    -0.2676176007837057 0.07754679918289184 0.06017719935625791 0.026339201629161833 -0.023899999260902405 \
    -0.0005745818838477134 -0.14321500211954116 -0.08186554280109704 -0.052084797993302345 -0.09267002018168569 \
    0.07356880605220795 -0.13853259906172752 -0.07658620160073042 -0.030316420830786228 0.1445817705243826 \
    -0.2317059963941574 0.2405868012458086 0.025874840561300516 -0.009670199826359749 -0.1255566030740738 \
    -0.04054120033979416 -0.1269154004752636 -0.07821047916077077 -0.11069419849663972 0.07569745955988764 \
    -0.09774660468101501 -0.15719179958105087 0.09785400182008744 -0.015729400515556335 -0.0534697987139225 \
    0.12641480267047883 0.11053360179066658 0.01566291986964643 -0.07557800635695458 0.05108000189065933 \
    -0.09036719799041748 0.15396699942648412 0.06501959543675184 -0.02553799916058779 -0.026882800459861755 \
    0.20645319819450378 0.07472440004348754 -0.004740800708532333 0.043976600468158725 -0.0021179988980293274 \
    -0.05375500172376633 0.04409979917109012 0.012081798911094666 -0.0731097985059023 -0.15731339305639266 \
    0.1089569978415966 0.09528559818863869 0.1060018421150744 -0.17337840497493745 0.0005883991718292237 \
    -0.13214479815214872 0.08728240057826042 -0.10987420007586479 0.04427489708177745 0.08991800155490637 \
    0.0307010801974684 -0.09979681838303804 -0.030030998587608337 0.07037360146641732 0.1252960187382996 \
    0.08624913953244687 0.018985400348901747 -0.013756000995635986 -0.0888078011572361 -0.13834053929895163 \
    -0.04176180101931095 -0.13501039743423462 0.05329440012574196 0.024851399660110473 0.061729401163756845 \
    -0.06209240108728409 -0.08478620126843453 0.18648139983415604 0.2723780035972595 0.3713199943304062 \
    0.06699139922857285 0.037641181517392396 -0.0298505999147892 -0.02204279825091362 0.1343094002455473 \
    0.16395820155739785 -0.047920739464461806 -0.03534359894692898 -0.022939597070217133 -0.02785577652975917 \
    -0.10899659842252732 0.024072401225566864 0.019510798156261444 0.1386731404811144 0.0028463996946811675 \
    -0.06617039963603019 -0.009488601982593537 0.12188920006155968 -0.03753960058093071 0.029703337862156332 \
    0.06211328096687794 0.11650202153250575 0.1277493990957737 -0.08780520185828208 -0.15254195807501675 \
    0.1925929993391037 -0.05928299650549888 -0.14749441877938807 -0.16375999748706818 -0.039694001711905 \
    -0.2939519971609116 0.08541699945926666 -0.005393202602863312 0.1128253996372223 -0.05797660034149885 \
    -0.14221079871058465".split()
    germany_berlin = "-0.06282061518868431 -0.08724112895750163 -0.008859959478017921 -0.06368025486353061 \
    -0.1013573255054277 0.17905025007943484 -0.013137434221121468 -0.11817432113296814 0.11056747280396526 \
    -0.00739817849127205 0.1263998086926527 -0.042477047375400624 -0.1480092867289921 -0.12576830562062094 \
    0.04970903851194065 0.08143301634949057 0.00014838737931724817 -0.09659003191333435 0.04346716113404948 \
    -0.022740745246987503 0.04924442664081935 -0.0601061653618046 0.014698657578589234 0.043108600239119776 \
    -0.07197682225239509 0.02129910582223721 -0.04688515464540286 0.010706445414361875 -0.18889107526053334 \
    0.2142334311532873 -0.04010156033638615 -0.03803496378300427 -0.07923789895425884 0.021243350546225715 \
    0.047511373759106815 -0.12890060050298388 0.0005081185929664922 0.028971446408673638 -0.05154755005049183 \
    0.18526404085974282 0.13903548451009032 -0.14984872354316375 0.06472565632383562 0.112444876786618 \
    -0.07864308646179251 -0.08868268874067034 0.339641325297955 -0.05025637979744595 -0.022671163598144223 \
    0.06522398094619197 -0.09918591713867964 -0.07714012343802719 -0.16919016159565195 0.032705274818806605 \
    0.27114011473224303 -0.11545044840470445 -0.004673374852282304 0.07098039411448746 0.19952612644044887 \
    -0.16349602412521907 -0.03871449964667098 -0.00991452436569402 0.07356107209926158 -0.01722387808151823 \
    -0.040957215299554005 -0.04340973862731333 0.04335673903346194 0.23812358297228184 0.07283901089905948 \
    -0.07525196008647707 0.12193499161140567 0.09491382892411143 -0.00449877530871495 0.07640651285496586 \
    -0.25918167452102986 -0.08396338268070463 -0.11078112450059535 0.07942456308475596 -0.07082887755037487 \
    0.08425283340558398 0.020352104273829868 0.2846300689426435 0.012652594467435692 -0.16436004608777113 \
    -0.0989109246442773 -0.01142742135511797 0.22485622953337717 0.04809235787085127 -0.2908704876286747 \
    0.07243085477159662 0.18231191175466185 -0.1498801502715025 0.10453376281230894 0.07330771265624363 \
    0.05412301061105976 0.07985096480118406 0.08947476531144534 -0.005564286030736019 0.130315392992044 \
    -0.028371474868364607 0.004460233817818217 -0.03147163498883885 -0.052367488519043624 0.002608315204826308 \
    -0.010207252752497068 -0.07292607909729883 -0.07963215353683176 0.04618650091839636 0.09953933329740823 \
    0.0488406665766977 -0.011046837592848939 -0.09450329650313784 0.02898107953812045 -0.020185651859638554 \
    -0.008826954240497694 0.12089972945834268 -0.08592394786619807 -0.10764047882587385 0.1759250161036327 \
    -0.05257214308325092 0.02934941299949998 -0.022362038686038824 0.0007941980073584398 0.15472672874830676 \
    -0.1259961084407337 0.079107618687841 0.18265689831118806 -0.13547152929668435 0.15808300399057115 \
    0.050389000057902006 0.20702162187005907 -0.10137842141702695 0.058853982552115346 -0.06024606626240541 \
    -0.04640260010583508 -0.03668388742540153 0.1406576566251018 -0.025362542990918953 0.0007021808410394588 \
    -0.12153927403787149 -0.15999263215158407 0.1595369980611439 -0.07778391026683654 0.0877607325466415 \
    0.1552536541164997 0.005563364285793286 0.20961148744131286 0.003689737706875227 0.07095678522671543 \
    -0.0882803178272475 0.004713291527955267 0.030270114427975194 0.09726074779983755 0.07019166283372734 \
    0.06979423290231777 0.053901876367538605 0.1277990256155141 0.0479645388601721 -0.13009648126139228 \
    0.001652543049207676 -0.2151624924585326 0.007541236570017213 0.07825575073804097 0.04153733115521054 \
    0.04644669334573726 -0.03595508629955562 -0.026373359698474368 -0.07920265148024049 -0.14336801752126271 \
    0.037105348914075194 0.017000136473510244 0.11435748207397811 -0.1413217227242471 0.03784036451700784 \
    -0.042231634496186894 -0.047855870191004395 -0.011260671045379617 0.09132001272422956 0.010527528389942814 \
    0.08668436326351978 -0.19060820738275558 -0.025106280376171228 0.05308317681575528 -0.04712320072210289 \
    -0.17711025915219106 0.015352245361021187 -0.007680454737105903 -0.0946701757521544 -0.05042815247073329 \
    0.09305828229726547 -0.1388934214385925 0.005904576473168914 -0.08633298453180473 0.0022988083334320826 \
    0.0034370911532863205 -0.0553592439071261 -0.0602818580572701 -0.006754229375224999 0.043941072452105996 \
    -0.12171844041798563 0.09079322558909263 -0.06991993829017792 -0.03886148365177516 -0.11265166091239028 \
    -0.09084342653225319 -0.1274232914694134 -0.0789409575955705 0.03560755925758859 0.031397832892531126 \
    -0.13152650700190754 -0.13919373286649242 0.11847788134328784 0.10034048853402155 -0.014794534639773729 \
    0.16960421956423632 0.09926767127994733 0.09145555443304933 -0.08564949761531755 0.002699911624143511 \
    -0.14773132378357065 0.058765718034653226 -0.09719229711866255 -0.07087860140475423 0.010344017089162647 \
    0.08444603937414961 0.1682283655994006 -0.03400277893027195 0.046727251905680736 -0.062219630406608765 \
    -0.11124934572352341 0.08045048583491711 -0.004233105898487002 0.008506014910867664 -0.12086081671466696 \
    0.04254866531069317 0.11658677551092089 0.05703815563579165 -0.10863339171123457 0.11296830014184793 \
    -0.05276083865678079 0.14679910326359255 -0.032563946177160186 0.059495648792897884 0.02044805704294447 \
    0.011418509486788462 -0.0214967615366073 -0.06779601433033355 0.05221580711834686 0.001067913860167775 \
    0.042125259071905875 0.0015305581610998824 0.02818376283547416 -0.17259820445053647 0.045557124115207334 \
    -0.02857631177678619 -0.07699536589398064 0.12368421896517517 0.035474410511562186 0.048378843367814076 \
    0.036494693998890014 -0.026940194901890632 0.08731286628971384 0.18673933965191708 0.30267764177596523 \
    0.0232746830184535 -0.10617305149331406 0.08557514451748657 0.03010734000493178 -0.06117825478393458 \
    0.011464370649484443 -0.0654344002741207 -0.035344711398610555 0.07897543719782585 -0.03277687328248957 \
    -0.028892247248087533 -0.03682996940197146 0.009722928688331876 0.061802493589464325 0.01260769584273273 \
    -0.06883250125874471 0.02599033325991021 0.23028594496865515 -0.09077156737684436 -0.025284078402088555 \
    0.04031858670319734 0.009805501169451432 0.10108535191349585 -0.09955546756062528 -0.24642450173800434 \
    0.10251893241659416 0.048040560758170904 -0.062490497463290835 -0.14904777593548596 -0.0071403928418177125 \
    -0.2118530988505638 0.10057068715379806 0.1175260616533694 0.20101287681356048 -0.0504712230688917 \
    -0.03066836348935001".split()
    china_beijing = "-0.07635857918552019 -0.07405002944772959 0.022606491093604738 -0.05643856407532644 \
    -0.08108868869800641 0.15138633679339278 -0.04468251710366893 -0.11292810802205833 0.10579423698158019 \
    -0.0362527198086266 0.1201502670965774 -0.045668077131439666 -0.1470557331696014 -0.13983635129216568 \
    0.053048013799815316 0.07809539681681107 -0.04156110483075456 -0.11074444656345386 0.06567505362443349 \
    -0.054658374165937086 0.036541808819920495 -0.05109642315467968 0.011190853217951473 0.03522237470986477 \
    -0.07436330441001199 0.027127044474015944 -0.050998372779668524 -0.02603552883950013 -0.15477468195183477 \
    0.20580367154648457 -0.03310249730931059 -0.03412700913292754 -0.058036180284492755 0.007328738554680223 \
    0.016873332214573513 -0.123381508374349 -0.0730709998345484 -0.005975873164246074 -0.020645907208021213 \
    0.17300286247102972 0.10948175104026678 -0.13885081919950565 0.06762290702188821 0.12396599700614236 \
    -0.08301037370939406 -0.09157258084286325 0.35554389506158074 -0.04366969701841045 0.008678214618972339 \
    0.03610611535251703 -0.03583360665496829 -0.14104484201264483 -0.16760104833345646 0.015172659666194896 \
    0.265132387837296 -0.09475340811452017 0.017371822711308514 0.05104824950350741 0.16297551443261135 \
    -0.1612219312852251 -0.035186831177673204 -0.028830275391342346 0.06910481032735286 -0.035982909089196324 \
    -0.044458298018056557 -0.0858214527197629 0.02255300126327935 0.22814300557512646 0.057688097321221425 \
    -0.10166597218790194 0.08034921949103771 0.08146850448265654 0.010594439078221215 0.08141178823617519 \
    -0.2598851964673756 -0.09256669093406454 -0.09589815055139508 0.08255576438748181 -0.05979629100243071 \
    0.07579408950623825 0.03349834417212156 0.2550508987211959 0.03940193318740429 -0.16387721546043577 \
    -0.08514244401142464 -0.01912316820772654 0.17524244018012855 0.07088078180996758 -0.27500663819502136 \
    0.08209996418800529 0.17013156502904228 -0.1594854414664221 0.1037995463516413 0.07033600824315693 \
    0.056130773530287635 0.055885388115973926 0.07725362320936759 -0.0076580807974628105 0.1338466694686827 \
    0.009242925713330954 -0.011663702932845257 -0.03189611945912835 -0.04322153163330293 -0.014678500598354198 \
    -0.008693355291131993 -0.04798516532170252 -0.07719955107721754 0.0667795420017045 0.11385067875476025 \
    0.04732546557764653 -0.0040610605365981305 -0.06375062041511846 0.030183553501069327 -0.009473715801762086 \
    -0.0159378730381883 0.10631245455536918 -0.07699255163170399 -0.07132921172503304 0.16524708145540778 \
    -0.06911285017471418 0.00666047634977511 -0.03197131912634482 -0.008133937827422054 0.15472449804929203 \
    -0.11982705989213358 0.09977421140841727 0.1927237097799294 -0.14124234085348333 0.1598887694351347 \
    0.042558269026003126 0.18029477563757043 -0.09072995590349942 0.02979260014828864 -0.06067656079877046 \
    -0.02907837453643388 -0.05009727540279358 0.13280566276794512 -0.060597893584469174 0.01592903150253867 \
    -0.11435177146907784 -0.12484066998217921 0.14893931043642078 -0.1002834081070331 0.12267446462407784 \
    0.14781010254200655 0.02077010495255452 0.14981161985103897 0.008784821706392124 0.06532242246765115 \
    -0.08402027915869822 0.016399149150522062 0.07175088633766522 0.08082848077718507 0.06584595591218317 \
    0.10288944835209414 0.04500081003838745 0.14959267298926182 0.08062092393127128 -0.10838129563023892 \
    -0.00406335757074208 -0.2061476724908909 0.026230678567243714 0.08845145189573055 0.04646326308997317 \
    0.06353983810158473 -0.04855630353571945 -0.023657920660937984 -0.09958972819161145 -0.17250957463823924 \
    0.042216398522209264 0.008292828664578466 0.13696165064061766 -0.1301303278267722 0.03759319534188603 \
    -0.009231011387798077 -0.0319152106502189 -0.03831410948991573 0.0651138944274917 0.01873818153014328 \
    0.0778773185802478 -0.16967405969427657 -0.018716274161822092 0.04673456534757572 -0.05651851676043908 \
    -0.15186561965242215 0.00010970145144786592 -0.012863318644406966 -0.08988854366728162 -0.09040675227724722 \
    0.07953261467831095 -0.17124500009933452 0.0007512746268056618 -0.07850430120719264 -0.0012756336758687416 \
    -0.032267938837053894 -0.07478026819179807 -0.06939174855315096 0.006231366951273766 0.040608618127818685 \
    -0.09334386209111242 0.1291773997786195 -0.026565928965828496 0.005130436631684752 -0.10827429410066929 \
    -0.08532236061064068 -0.13396490702546532 -0.05452585578071627 0.02165055264594846 0.04172586691874835 \
    -0.12071055254526386 -0.13134589228444532 0.07946884832700263 0.10527210645358556 -0.015558560190120314 \
    0.11687849758139944 0.11312822473607256 0.11626061710509263 -0.09216872356393015 0.0009729652109595685 \
    -0.1706317480859206 0.0673040942077217 -0.08936553514065358 -0.0697526164787778 0.02644136616671663 \
    0.04988705029845215 0.15982663696434116 -0.031941883267989545 0.045785523336010395 -0.04410423819142522 \
    -0.09971191371226941 0.07232629925711706 0.0019867245175073517 -0.017955112629030216 -0.14073061935081532 \
    0.060028163102092115 0.12665110034542834 0.09906869862040465 -0.11464057196645304 0.12247353640944467 \
    -0.058938606667568436 0.16132928441455793 -0.02636293441051638 0.060694559789213134 0.0510131874077499 \
    -0.002988653009707856 -0.024525456678754467 -0.05717603363095788 0.07414892485627785 0.01649113839165288 \
    0.015876485351759596 0.007351882302918917 0.014236123444215223 -0.13822976922910246 0.06743828048097822 \
    -0.009742432046629846 -0.04450285747542943 0.10710754137818262 0.028596129552820645 0.06552171329097534 \
    0.04427706717447443 -0.03634378915327471 0.09186543304228759 0.1855395988012979 0.2776866412427815 \
    0.038992539542526024 -0.12294587961328378 0.047971153370857146 0.022788062685245348 -0.059848362303416267 \
    0.0570835061571816 -0.064031705332966 -0.0040911778812394274 0.08919971176757928 -0.0264286700640715 \
    -0.03793162032945581 -0.03408033861158754 0.009217534507876903 0.06617054240307674 0.015232417385780348 \
    -0.07962393894620155 0.046108914627796184 0.19896338354956125 -0.07948450841182007 -0.05293856030641118 \
    0.021380957313270447 -0.01773381316182361 0.09387088327503837 -0.11112647522943998 -0.2562868407819108 \
    0.09973082158254658 -0.017858450439958026 -0.08869436260990175 -0.09530723077524804 0.03965628450757484 \
    -0.2340747032235469 0.09623151546788491 0.1304843091204 0.1712862814225863 -0.048009233422562865 \
    -0.05803377450829645".split()
    germany_german = "-0.06747994094131919 -0.043182569539532985 0.03443403170123195 -0.03406880670644617 \
    -0.10590530920570561 0.16277501431327446 -0.03233244708959076 -0.08538752591222175 0.1091079619961126 \
    -0.030599656653583236 0.08371807465613583 -0.014843040270186738 -0.09066934669555425 -0.1113253187188059 \
    0.0749355949870681 0.10590945999106574 -0.038353285962756056 -0.09868963502631119 0.07334130262838232 \
    -0.01920657629910732 0.032320153139270676 -0.035721825524445 0.06942682113432101 0.05398969587040118 \
    -0.08019527812499329 0.023072162576132003 -0.04169300581522404 -0.006844964080230985 -0.151859882527859 \
    0.18985525327440897 -0.023038735430268648 -0.056215007306895944 -0.05159447954781863 0.007758786687866639 \
    -0.0041355807520632145 -0.07296577279612632 -0.043250058406837405 -0.010741189252577589 -0.022802436988942673 \
    0.1572452570167034 0.06997687386318523 -0.11201205525796962 0.07960133485857229 0.13163517251846313 \
    -0.08738813270138045 -0.07000572952105288 0.3055852886193398 -0.04931306581168709 0.07588234947945272 \
    0.03394340477232295 -0.057494451572664494 -0.11900250264906141 -0.17520280808747976 0.008322866887057112 \
    0.26100773901258645 -0.10709282080306824 0.03785040486966427 0.0675871391191181 0.10666392307853823 \
    -0.15083933134939592 -0.03318112395896965 -0.03126231874380402 0.06780233623349254 -0.02929041839408852 \
    -0.046750678623370484 -0.0921436513730069 0.004979763205291118 0.1929558875584553 0.06314744014818006 \
    -0.08886149836345092 0.03623013896105075 0.11023795992161077 0.04110478941282218 0.08734350363253973 \
    -0.19645253504743787 -0.07482034688027216 -0.09452433641071879 0.056542232434038 -0.07391831296452685 \
    0.0747733195743069 0.07933780444095642 0.2523222078561427 -0.016874703501855725 -0.1559849130942384 \
    -0.10451933205085664 0.03440842407303529 0.18585690719027367 0.1154216556830465 -0.2666709845620868 \
    0.04413321149482036 0.1653628695668969 -0.17224683638244004 0.1116299165246134 0.06789663591516319 \
    0.051843435196877244 0.05606622065645554 0.07117445686707292 -0.02613079248836917 0.18990539206203474 \
    0.0014088844056471233 -0.03602027007903634 -0.0399718449004752 -0.034063317480489005 -0.019649450037827792 \
    0.002919091445703559 -0.025301670062583714 -0.05620698438518904 0.05893931919930379 0.12083696619531363 \
    0.01959556216531658 0.016268367600828625 -0.06012506504304785 0.02296501659161905 0.003693629119446367 \
    0.05312361551550166 0.06999866677215531 -0.10045962121388081 -0.04960043307931003 0.1789239157966733 \
    -0.038650319164149294 -0.007491486874855798 -0.049023243719765616 -0.01571006088898525 0.17376298038034302 \
    -0.07529071447427636 0.11032932942668101 0.1801755667573856 -0.20531148772958124 0.17502632934237816 \
    0.044828425318598916 0.1610371947326349 -0.07870867954083281 0.047449346060142616 -0.060195366222813414 \
    -0.023254217543822814 -0.05909378564424896 0.13700534929369426 -0.03405706402600976 0.04428682177627977 \
    -0.13520459044304733 -0.11373785939908712 0.15398645749136503 -0.0867824051340756 0.11183271180439493 \
    0.14149712954409666 0.02311291044172781 0.16603415211684885 -0.0032449492013240876 0.05923589440372223 \
    -0.05186956804171194 0.021962718915822146 0.09573389342538427 0.046751589997492723 0.021111969120446755 \
    0.10758955765871603 0.0030043537838920194 0.13733260378192919 0.09489772018875262 -0.1141672741190136 \
    -0.0010959866286232966 -0.21811796709035836 0.02075650826154812 0.05639255813333481 0.0756953180454039 \
    0.07151400316513094 -0.015130550070450536 -0.00781935476935063 -0.13113011178076156 -0.18402307458015577 \
    0.05079056426523576 0.019928937581285108 0.13855333433480466 -0.10689851881274882 0.045155397400848535 \
    -0.008305840368971792 -0.016098198404843122 -0.03008208315298694 0.0789161723483383 0.03444551403726147 \
    0.07669763010963421 -0.2146578185607997 -0.033809921545431394 0.0445926567115219 -0.09871763257491625 \
    -0.13021514738052217 0.024126408184700857 -0.04762457493298305 -0.05915886779796432 -0.08186689269688593 \
    0.058591318390952704 -0.1556572303679632 0.018843298921964628 -0.07413753663109446 -0.005793019209117819 \
    -0.06395314032385674 -0.06494039090606535 -0.06917393487063014 -0.024029349840919752 0.08378950153428606 \
    -0.12379021062729414 0.11140525159235881 -0.006351163556203705 -0.0077219941240524595 -0.11255951260220903 \
    -0.08582251901079345 -0.13344112987148155 -0.01695629534681172 -0.030389420622332988 0.05809221688751565 \
    -0.10838342965959576 -0.09480088348948643 0.0762776299737041 0.10328228969714708 -0.04201049301223377 \
    0.09938108954383017 0.12507386864300674 0.10440543844079347 -0.07584281742120036 0.0778967713675178 \
    -0.16867028019435976 0.03022802045873483 -0.08877758678310275 -0.0933766886722715 0.060499484316656735 \
    0.05160574481778077 0.11273888485095217 -0.00629761116289737 0.0710398739821439 -0.035560767746660664 \
    -0.07193278096782459 0.07297096216220322 -0.00724606496755087 -0.03472194861133532 -0.1210563049858415 \
    0.057113341239297476 0.16565811382901074 0.07874199963594056 -0.1296972171056977 0.11822721655704321 \
    -0.06493180643135239 0.13930857567193786 -0.03424442794494098 0.08468492414672887 0.030590924310072466 \
    0.010520773791437428 -0.04572381397582955 -0.04593025544464143 0.04711959558854928 0.04977062583715797 \
    0.008091584141556545 0.00518913285205718 0.015196242162081347 -0.15440018893381338 0.01477741822100284 \
    0.01128320896741116 -0.03302683178172898 0.08858977960752261 0.05576105369246598 0.07974541084989509 \
    0.012198985462868352 -0.05909014432072781 0.1013534460217352 0.1821444500856377 0.27669271000860446 \
    0.027118185274347697 -0.10609399678399377 0.01943705669492717 0.02452283258493206 -0.056936445720667875 \
    0.08628014550087301 -0.08086996824757675 0.02318956625706621 0.09354908342841742 -0.05840980070881635 \
    -0.07340019954941197 -0.029512655164787783 0.038304374026778165 0.1020823202309442 0.014574800771394173 \
    -0.051556779047009886 0.0361489385484358 0.15871218870991982 -0.09929301078626994 -0.01878642425707002 \
    0.017942854375520297 -0.03716860039533156 0.1032960227931796 -0.1202210405657447 -0.23404143273403716 \
    0.1365643397606046 0.008414470449772633 -0.14196520423813694 -0.0983026755669912 0.06884104216742606 \
    -0.20796302808098285 0.14372745464365724 0.16559802615141006 0.15835736819737953 -0.053516890354376706 \
    -0.0871673172580796".split()
    cos = torch.nn.CosineSimilarity(dim = 0, eps = 1e-6)

    # Unrelated pair: 0.7621 -- 25e: 0.8928 -- 50e: 0.8974
    print(cos(torch.FloatTensor([float(i) for i in fresh_rotten]), torch.FloatTensor([float(i) for i in germany_berlin])))
    # Related pair: 0.9781 -- 25e: 0.9873 -- 50e: 0.9845
    print(cos(torch.FloatTensor([float(i) for i in germany_berlin]), torch.FloatTensor([float(i) for i in china_beijing])))
    # Not related but close: 0.9696 -- 25e: 0.0.9845 -- 50e: 0.9833
    print(cos(torch.FloatTensor([float(i) for i in germany_german]), torch.FloatTensor([float(i) for i in china_beijing])))


def main():
    trainmodel_getembedding()

if __name__ == '__main__':
    trainmodel_getembedding()
    # checkTensor()
    # loadmodel_calculateembedding()
